{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02507150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import re, os, pickle, faiss, openai, numpy as np, difflib\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b31b458",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_index = faiss.read_index(\"gene_meta_index.faiss\")\n",
    "with open(\"gene_docs.pkl\",\"rb\") as f: gene_docs = pickle.load(f)\n",
    "ccre_index = faiss.read_index(\"ccre_meta_index.faiss\")\n",
    "with open(\"ccre_docs.pkl\",\"rb\") as f: ccre_docs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecae0cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "ID_REGEX = re.compile(r\"\\b[A-Za-z][A-Za-z0-9\\.-]*\\b\")\n",
    "ACCESSION_REGEX = re.compile(r\"\\b(EH\\d+E\\d+)\\b\", re.I)\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffda337",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"gpt2-igscreen\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_DIR)\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_DIR, use_safetensors=True)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5b6465",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPT-2 Prompt Generator Ready (type 'exit' to quit)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebec9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LINEAGE_TERMS = [\"hematopoetic\", \"atac\", \"dnase\", \"progenitor\", \"erythroblast\", \"plasmacytoid\", \"myeloid\",\n",
    "                    \"monocyte\", \"macrophage\", \"natural killer\", \"double negative\", \"immature\", \"mature\", \"memory\",\n",
    "                    \"effector\", \"regulatory\", \"helper\", \"plasmablast\", \"b cell\", \"cd8\", \"t cell\", \"double positive\", \n",
    "                    \"stem cell\", \"gamma delta t\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18261a10",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "PLOT_TERMS = [\"plot\", \"graph\", \"upset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a133a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_lineage(raw: str) -> bool:\n",
    "    text = raw.lower()\n",
    "\n",
    "    has_plot_term = any(term in text for term in PLOT_TERMS)\n",
    "\n",
    "    has_lineage_term = any(term in text for term in LINEAGE_TERMS)\n",
    "\n",
    "    return has_plot_term and has_lineage_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5595c71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea98daf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def rag_accession(accession: str) -> str:\n",
    "    url =f\"https://igscreen.wenglab.org/icre/{accession}\"\n",
    "    return(\n",
    "        f\" cCRE annotation for {accession}: \\n\"\n",
    "        f\"{url}\\n\"\n",
    "        \"You can further filter by Biosample, Stimulation, and other columns; \"\n",
    "        \"Explore what genes and variants are associated from the tabs on the left\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78288993",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def rag_lineage() -> str:\n",
    "    url = \"https://igscreen.wenglab.org/lineage\"\n",
    "    return(\n",
    "        \"Explore the UpSet plot comparing iCRE activity \\n\"\n",
    "        f\"{url}\\n\"\n",
    "        \"Use the filters to select between 2 to 6 cells to compare.\"\n",
    "        \"You will also be able to see different active iCRE depending on the filters\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9e3d01",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "VALID_ACCESSIONS = {doc[\"id\"].upper() for doc in ccre_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343313bd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def ensure_accession(raw: str) -> str | None:\n",
    "    for tok in re.findall(ACCESSION_REGEX, raw):\n",
    "        up = tok.upper()\n",
    "        if up in VALID_ACCESSIONS:\n",
    "            return up\n",
    "        # 2) Optional fuzzy:\n",
    "        matches = difflib.get_close_matches(up, VALID_ACCESSIONS, n=1, cutoff=0.8)\n",
    "        if matches: return matches[0]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231cadc1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def call_openai_fallback(prompt):\n",
    "    fallback_prompt = (\n",
    "        \"You are a bioinformatics helper trying to provide redirection. \"\n",
    "        f\"Try to answer your best from:\\n'{prompt}'\"\n",
    "    )\n",
    "    try:\n",
    "        resp = openai.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\":\"user\",\"content\":fallback_prompt}],\n",
    "            temperature=0.0\n",
    "        )\n",
    "        return resp.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"(OpenAI fallback failed: {e})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11daca6b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "VALID_GENES = {doc[\"id\"].upper() for doc in gene_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33acc36e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def normalize_gene(q: str) -> tuple[str,str] | None:\n",
    "    # 1) exact token\n",
    "    for tok in re.findall(ID_REGEX, q):\n",
    "        up = tok.upper()\n",
    "        if up in VALID_GENES:\n",
    "            return tok, up\n",
    "\n",
    "    # 2) fuzzy\n",
    "    for tok in re.findall(ID_REGEX, q):\n",
    "        up = tok.upper()\n",
    "        matches = difflib.get_close_matches(up, VALID_GENES, n=1, cutoff=0.8)\n",
    "        if matches:\n",
    "            return tok, matches[0]\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342a96ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input = input(\"\\nEnter your question: \").strip()\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    # Normalize gene before generation\n",
    "    norm = normalize_gene(user_input)\n",
    "    if norm:\n",
    "        orig_tok, gene = norm\n",
    "        user_input = re.sub(\n",
    "            rf\"\\b{re.escape(orig_tok)}\\b\",\n",
    "            gene,\n",
    "            user_input,\n",
    "            flags=re.IGNORECASE\n",
    "        )\n",
    "    acc = ensure_accession(user_input)\n",
    "    if acc:\n",
    "        answer = rag_accession(acc)\n",
    "        print(\"\\nAssistant:\\n\" + answer)\n",
    "\n",
    "    elif ensure_lineage(user_input):\n",
    "        answer = rag_lineage()\n",
    "        print(\"\\nAssistant:\\n\" + answer)\n",
    "    else:\n",
    "        prompt = user_input + \" => \"\n",
    "        tokens = tokenizer(\n",
    "            prompt, return_tensors=\"pt\",\n",
    "            padding=True, truncation=True, max_length=512\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in tokens.items()}\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=50,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                do_sample=False\n",
    "            )\n",
    "        full_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        if full_output.startswith(prompt):\n",
    "            full_output = full_output[len(prompt):].strip()\n",
    "\n",
    "        # extract the URL\n",
    "        m = re.search(r\"https?://\\S+\", full_output)\n",
    "        if m:\n",
    "            answer = m.group(0)\n",
    "            print(\"\\nAssistant:\\n\" + answer)\n",
    "        else:\n",
    "            print(\"\\nFine-tuned model could not generate a useful response.\")\n",
    "            print(\"\\nAssistant (OpenAI fallback):\")\n",
    "            answer = call_openai_fallback(user_input)\n",
    "            print(answer)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
